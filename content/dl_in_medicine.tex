\chapter{Deep Learning in Digital Histopathology}
Manual analysis of histopathology slides is expensive, takes long time to complete and requires highly trained professionals and quality assurance by performing peer reviews \cite{Wemmert2021}. With the invention of virtual microscopy, which enables H\&E stained glass slides to be converted into digital slides, and the introduction of Whole-slide Images (WSIs), the field is entering a new era. The term Digital Pathology or Digital Histopathology is often being used. In Digital Pathology, much effort is put into developing tools that would help medical experts to semi- or fully automate the visual analysis of the digital slides. Entities such as different tissue types and cells can be identified and classified.

Deep learning has shown extreme potential in many areas, including medicine and processing of medical image data \cite{LeCun2015}.

\section{Architectures}
Neural network architectures like Convolutional Neural Networks \cite{LeCun2015-2} and U-Net \cite{Ronneberger2015} have proven to be effective in medical image analysis \cite{Santosh2022-2}. In recent years, also a concept of Vision Transformers \cite{Dosovitskiy2020, Hu2023} used in medical imaging shows promising results \cite{Shamshad2023, Hu2023, He2023}. In further sections we describe each architecture and its contribution to the analysis of medical images and Digital Pathology.

\subsection{Convolutional Neural Networks}
In 1959, Hubel and Wiesel conducted experiments that inspired the advent of the Convolutional Neural Networks (CNN). In their experiments, they put a microelectrode into a cat's brain (into the part called primary visual cortex), while it was under partial anaesthesia. While showing various images to it, they measured the neurological activity of the cortex \cite{Hubel1959}. According to the results, a hierarchical pattern can be observed in the activity of the visual cortex, where the neurons close to the retina captured the simplest patterns (like different illuminations and lines under various angles) and the farther layers  captured more complex patterns (like geometric shapes and other complex visual patterns) \cite{Hubel1959}. 

CNNs took advantage of these findings and rebuilt the classical neural network layers to be able to capture more complex features with increasing depth. They utilize so-called convolution layers along with the ReLU activation function to learn to extract relevant features from the image. The deeper the convolution layer, the more complicated features it can learn \cite{Santosh2022-2}. 

Similarly to the Hubel and Wiesel cat's visual cortex, the first layers can learn to identify basic shapes like lines and simple geometric shapes, and the deeper layers can learn to identify more complex ones. Such an example of CNN is shown in figure \ref{fig:cnn}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=10cm]{assets/images/cnn.jpg}
\par\end{centering}
\caption{Example of CNN learning strategy \cite{Santosh2022-2}}
\label{fig:cnn}
\end{figure}

A convolution layer applies a series of operations on its input and produces an output map. The most important part is applying a kernel, which is a tensor of fixed width and height, over the input image or output map from the previous convolution layer. This fundamental operation serves as a feature-extracting technique. The kernel slides over its input across its height and width and at each step it performs element-wise multiplication of the pixel values it currently overlaps at each layer of depth and then sums them together to produce a single value. So, for example, if the input is of size $100\times100\times3$ (standard RGB image with 3 channels for red, green, and blue colour), then the kernel is applied to all three channels simultaneously. After the kernel is applied to the whole image, the resulting output map will be of size $100\times100\times1$ (assuming that other hyperparameters of convolution are configured in such a manner that original width and height remain unchanged for the output map - we will cover them later in this chapter), because the kernel will collapse its depth. 

The kernel filters can be handcrafted to multiply and intensify certain properties of the image. Examples include the Prewitt, Gabor, Sobel, Laplacian, and Roberts filters for edge and gradient detection. In standard image processing, the weights inside of the kernel are preset. However, in the CNNs these weights are learned during training, so the network determines what features of the image the output maps will be focused on and hence the network can be more effective \cite{Santosh2022-2, He2023}. In the AlexNet \cite{Krizhevsky2012}, the first deep CNN which outlined the original structure, a ReLU activation function was applied to the value obtained from the convolution operation to break the linearity of the operation. Since then, using an activation function after the convolution operation has become a standard practice \cite{Santosh2022-2, He2023}, and the name of the output produced by the convolution + ReLU is also called an activation map.

According to the \cite{Santosh2022-2}, convolution operation can be expressed as a function with hyperparameters: $\varphi_{\text{conv}}(C_{\text{in}}, C_{\text{out}}, K, S, P, D)$. The definition of these hyperparameters is:

\begin{itemize}
    \item $C_{\text{in}}$ is the number of channels of the input map - its depth.
    \item $C_{\text{out}}$ is the number of channels of the output produced by the layer - it is also the number of filters that will be applied to the input map, since one filter produces activation map with one channel.
    \item $K$ is a tuple that defines the size of the kernel - its width ($k_w$) and height ($k_h$)
    \item $S$ is also a tuple, which defines the stride - the number of pixels the kernel will slide along with, both in terms of width and height.
    \item $P$ can also be a tuple and it defines the number of added dummy pixels to artificially increase the input map size in order for the output map to keep the same size as the input map (otherwise the output map would be smaller since the kernels cannot slide outside of the boundary of the input map).
    \item $D$ (a tuple as well) is the dilation and it serves the purpose of increasing the field of view of the kernel (the area of image it can cover) without adding more weights into it. Dilation defines the gap that is added both horizontally and vertically between the weights of the kernel.
\end{itemize}

We can see an example of the convolution in the figure \ref{fig:convolution}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=8cm]{assets/images/conv.jpg}
\par\end{centering}
\caption{Example of the convolution operation \cite{Santosh2022-2}}
\label{fig:convolution}
\end{figure}

The field of view of a single kernel is small, as typically kernels of size $3\times3$, $5\times5$, or $7\times7$ are used \cite{Santosh2022-2}. In order to address this issue and to be able to build up and capture more complex features in the subsequent layers, the pooling layer is often added. The pooling layer effectively downsamples the feature maps, commonly by a factor of two. This allows the next convolution layers to learn more abstract features that were further apart in the previous feature map. In order to compensate the information lost during the downsampling, usually a number of independent kernels is increased for the next convolutions after each pooling layer. During pooling, a small array slides over the input map and always selects only a single value from the area it covers, hence decreasing the size of the map. Two pooling methods are common:

\begin{enumerate}
    \item Max pooling selects the maximal value from the area it covers, and
    \item Average pooling computes the mean from the values it covers.
\end{enumerate}

Example of pooling, both max and average, can be seen in the figure \ref{fig:pooling}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=8cm]{assets/images/pooling.jpg}
\par\end{centering}
\caption{Example of max and average pooling operations \cite{Santosh2022-2}}
\label{fig:pooling}
\end{figure}

CNNs are usually organised as repeating layers of convolutions, followed by the ReLU activation function and then the pooling layer. The output can be then fed into another convolution and a deep CNN can be build using this approach. There is a rule of thumb \cite{Santosh2022-3}, where we start with a small number of independent filters with a small field of view and then we downsample the image by the factor of two (to increase the field of view) and double the subsequent number of independent filters as can be seen in figure \ref{fig:cnn-layers}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=12cm]{assets/images/cnn-layers.jpg}
\par\end{centering}
\caption{Scale space pyramid of a CNN \cite{Santosh2022-3}}
\label{fig:cnn-layers}
\end{figure}

In CNNs with very large depth, a concept of a skip connection, firstly introduced in the \cite{He2016}, can be used. A skip connection allows the input of a layer to bypass the convolutions and then be added to their result, as shown in figure \ref{fig:skip-conn}. Also, if we can represent a series of convolution with a function $\varphi(x)$, then the output with skip connection included would be given by $y=\varphi(x) + x$. This residual learning architecture can mitigate the problem of vanishing gradient in very deep CNNs \cite{Santosh2022-2}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=8cm]{assets/images/skip-conn.png}
\par\end{centering}
\caption{A building block of residual network \cite{He2016}}
\label{fig:skip-conn}
\end{figure}

Other strategies that can improve the performance of CNNs include batch-normalization layers \cite{Ioffe2015}(which are inserted after the convolution layers) and the utilization of parallel branches \cite{Szegedy2015} (which use different kernel sizes to compute multi-scale feature maps by concatenating their output maps).

After the series of convolutions and downsampling, the final compressed feature representation of the image needs to be flattened (converted into a vector) so it can serve as an input for the fully-connected layer(s), which will then make the appropriate decision based on the task the model should do. Two methods exist \cite{Santosh2022-2}, how we can convert the image descriptor into a vector:

\begin{enumerate}
    \item Reshape the activation maps to form a one-dimensional tensor \cite{Krizhevsky2012, LeCun2015-2}.
    \item Use average pooling on the entire activation map which will collapse the entire information into a single value and then concatenate these values to form a vector. For $N$ activation maps we will get a vector with $N$ elements \cite{He2016, Szegedy2015}.
\end{enumerate}

We can see their visual representation in figure \ref{fig:flatenning}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=12cm]{assets/images/flattening.jpg}
\par\end{centering}
\caption{Different flattening strategies \cite{Santosh2022-2}}
\label{fig:flatenning}
\end{figure}

An example of a CNN with all layers is in figure \ref{fig:full-cnn}. The input image features are extracted by convolution layer with four independent $5\times5$ kernel filters, followed by ReLU activation function and max pooling layer. Then the extracted image descriptor is flattened into a vector and fed into fully connected layers which serve as the classification output head.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=10cm]{assets/images/cnn-full.jpg}
\par\end{centering}
\caption{Example of CNN architecture \cite{Santosh2022-2}}
\label{fig:full-cnn}
\end{figure}

There are many CNN architectures that introduced new concepts in the field of computer vision and image analysis when they were presented. Considering today's knowledge and advancements, some of them might look trivial, but their contribution should not be overlooked. In the classification task, we can mention:

\begin{enumerate}
    \item LeNet5 \cite{LeCun2015-2}: uses series of convolution and pooling operations to extract the image features and fully connected layers to classify the input. It was introduced as a model that should recognize handwritten digits.
    \item AlexNet \cite{Krizhevsky2012}: built on top of the LeNet, utilized a deeper architecture along with ReLU activation functions and had immense success at the ImageNet Large-Scale Visual Recognition Competition. This success brought huge attention to the  CNNs and their potential in image analysis related tasks.
    \item VGGNet \cite{Simonyan2014}: introduced a strategy of expandable convolutional blocks, where in each block a varying number of convolutions can be used. Each block is then followed by the max pooling layer. This allowed for tailoring the network to reflect the complexity of the problem it was assigned to solve.
    \item GoogleLeNet \cite{Szegedy2015}: used kernels of varying sizes to extract features from the input maps and then concatenated their output maps to create a depth-wise combined feature. It also used the full-scale average pooling in the final feature extracting layer to create a one dimensional tensor. Furthermore it utilized auxiliary classifiers in the intermediate layers to boost the gradient flow. In the later version, called Inception Net, it introduced more concepts, like kernel factorization and batch normalization \cite{Szegedy2016-2}.
    \item ResNet \cite{He2016}: introduced a strategy of skip connections to solve the problem of vanishing gradient in very deep CNNs. 
\end{enumerate}

In the detection task, two main architectures were created, namely the Region-based CNN \cite{Girshick2014} (further upgraded to the Faster R-CNN and Mask R-CNN \cite{Ren2017}) and the You Only Look Once (YOLO) \cite{He2017}. 

Both R-CNN and YOLO are able to localize the object in an image with a bounding box label. However, sometimes we want to obtain a more precise location of the object, and this is where segmentation comes into play.

\subsection{U-Net and Its Variants}
In order to perfectly localize an object in an image, we would like to construct a pixel-level mask that would mark the pixels where the object is present. We can use classification for this - each pixel can either be classified as the one which does or does not display a part of the object. Segmentation algorithms have a deep impact and huge potential for medical imaging and digital pathology, where they can be used to mark different tissue types, organs, cells, and tumour regions \cite{Santosh2022-3}.

The limitation of fully CNN architectures is the inability to preserve the spatial information after the initial feature-extracting layers \cite{Santosh2022-3}. This issue is addressed in a new type of architecture, the encoder-decoder architecture. This architectural type, also known as the autoencoders or autoassociative networks, was firstly introduced in the \cite{Kramer1992}. Its visual representation is shown in figure \ref{fig:autoencoder}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=12cm]{assets/images/encoder-decoder.jpg}
\par\end{centering}
\caption{Example of the autoencoder architecture \cite{Santosh2022-2}}
\label{fig:autoencoder}
\end{figure}

It consists of two main parts:

\begin{enumerate}
    \item The encoder part, which is responsible for encoding the input image and extracting the most descriptive features, compressing them and reducing the redundancy, and
    \item The decoder part, which is responsible for reconstructing the original input image from the compressed image descriptor.
\end{enumerate}

The loss function computes the difference between the original input image and the image constructed by the decoder. If the decoder is able to create an image that looks very similar to the original, it means that the hidden representation of the image extracted by the encoder is credible enough. Then the decoder part can be removed and instead a classification, segmentation, or localization module can be attached and exploit the features learned by the encoder \cite{Santosh2022-2}.

In segmentation tasks, a simple change is added to the decoder part. Instead of generating the original image, it is trained to create the segmentation mask, where each pixel has a probability of belonging to a certain class. This computed probability distribution mask is used along with the original mask in the loss function to compute their difference and guide the training.

When the activation maps are downscaled in the encoder using operation such as max pooling, where only a single value is picked, we need a correct mechanism to reconstruct the original spatial position of that value during the upscaling of the maps. This can be achieved by remembering and forwarding the indices of the chosen value, as it was first introduced in the SegNet architecture \cite{Badrinarayanan2017}.

One of the most widely adopted and impacting architectures is the U-Net model \cite{Ronneberger2015}. U-Net was designed as the model for medical image segmentation tasks and achieved great success in doing so \cite{Santosh2022-3, Siddique2021}. Its architecture can be seen in figure \ref{fig:unet}.

\begin{figure}[H]
\begin{centering}
\includegraphics[width=12cm]{assets/images/unet.png}
\par\end{centering}
\caption{U-Net architecture \cite{Siddique2021}}
\label{fig:unet}
\end{figure}

Similarly to autoencoders, it has two main parts, the encoder and decoder. The encoder works as a classical CNN feature extractor, with convolutions, ReLU activation functions and max pooling. In each layer there are two convolutions (kernel size is $3\times3$), followed by the ReLU and the resulting activation maps are downsampled by the factor of two using the max pooling layer (with $2\times2$ matrix). Before the downsampling happens, the copy of activation maps is sent to the decoder (skip connections). Every next block doubles the number of channels. This is repeated four times, until we reach the bottleneck, where we again perform the convolutions with ReLU, but omit the downscaling. At this point, the features of the input image are in their most compressed form. Output from the bottleneck is then send to the decoder part. Next the decoder part uses up-convolutions (kernel size is $2\times2$), to expand the feature maps (double their size) and halve the number of channels. Particularly these up-convolutions are distinctive when comparing U-Net to other architectures \cite{Siddique2021}. The activation map is upscaled by up-convolution, then the activation maps, previously sent from the encoder, are concatenated with it. The concatenation is a required step, since the the border pixels are lost in every convolution (the convolutions do not use padding), and also to reintroduce some information that might be lost during the downsampling. Then again two $3\times3$ convolutions with ReLU are applied. This is also repeated four times, to reflect the encoder blocks. This approach can be visually drawn into a u-shape like architecture, from which the U-Net derived its name. Finally after the last decoder block, the $1\times1$ convolution is used to get the desired number of channels.

Since the original U-Net, many different variants of it were introduced \cite{Siddique2021}. To list a few examples:

\begin{itemize}
    \item 3D U-Net \cite{Çiçek2016}: works as a classical U-Net but was modified in a way that it can segment 3-dimensional data. Every 2D operation (2D convolution, 2D pooling, 2D up-convolution) were replaced by their corresponding  3D equivalents. It can be useful in medical images that utilize 3D space like MRI and CT.
    \item Attention U-Net \cite{Oktay2018}: utilizes the attention gate to draw attention of the network on the important parts of the image. These attention gates are inserted on a place where the concatenation of encoder feature maps and decoder feature maps should occur, as can be seen in figure \ref{fig:att-unet}. Before this concatenation happens, both sets of feature maps are run trough the attention gate where a series of operation is performed. These operations are visualized in figure \ref{fig:att-gate}. Firstly both sets are run trough a $1\times1\times1$ convolution to align their dimensions and then are added together. Then they pass through the ReLU activation function, $1\times1\times1$ convolution layer to reduce their depth to 1, sigmoid activation function to squeeze the values between 0 and 1 and an optional resampler to correctly align the spatial dimensions. This results in a attention map containing values between 0 and 1 and with the depth of 1. This attention map is then broadcasted and multiplied by the feature maps from the encoder - this produces the final output of the attention gate, which is then concatenated with the feature maps upsampled by the decoder.

    \begin{figure}[H]
    \begin{centering}
    \includegraphics[width=12cm]{assets/images/att-unet.png}
    \par\end{centering}
    \caption{U-Net architecture with added attention gates \cite{Oktay2018}}
    \label{fig:att-unet}
    \end{figure}

    \begin{figure}[H]
    \begin{centering}
    \includegraphics[width=12cm]{assets/images/att-gate.png}
    \par\end{centering}
    \caption{Additive attention gates \cite{Oktay2018}}
    \label{fig:att-gate}
    \end{figure}

    \item Residual U-Net is inspired by the \cite{He2016} and uses the skip connections within each block to help the gradient flow and address the problem of vanishing gradient.
\end{itemize}

Many more U-Net variants exists that we do not explain further in this work, for example the Inception U-Net, Recurrent U-Net, Dense U-Net, Adversarial U-Net, Ensemble U-Net, and $\text{U-Net}^{++}$, each showing potential in various medical imaging domains, from CT and MRI scans to radiology and histology \cite{Siddique2021}.
% architecture, advantages, limitations, how they can be used in medicine, limitations

\subsection{Vision Transformer}
% architecture, advantages, limitations

\section{Challenges, Strategies and Future}

\subsection{Data Annotation}
% how data is annotated, problems limitations etc

\subsection{Weakly Supervised Learning}
% what it is, how it works, challenges

\subsection{Active Learning}
% what it is, how ut works, why it is important, explainability

\subsection{Future Trends}
% what to expect next

